{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882bed7-36f6-4f84-9604-b7f6d2420959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SWAT+ Evapotranspiration Spatial Performance Evaluation Tool\n",
    "\n",
    "This tool  evaluates SWAT+ ET performance against reference data (e.g., WaPOR)\n",
    "and identifies soil-landuse combinations associated with poor model performance.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter, label\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===========================\n",
    "# USER-DEFINED INPUTS\n",
    "# ===========================\n",
    "\n",
    "# Main ET comparison files\n",
    "OBSERVED_ET_PATH = r\"path/to/your/ObservedET.tif\"  # Reference ET (e.g., WaPOR)\n",
    "SWAT_ET_PATH = r\"path/to/your/SWAT_modeled_ET.tif\"   # SWAT+ modeled ET\n",
    "WATERSHED_BOUNDARY = r\"path/to/your/watershed_boundary.shp\"  # Watershed shapefile\n",
    "\n",
    "# Water bodies mask (optional - set to None if not needed)\n",
    "WATERBODIES_MASK = r\"path/to/your/waterbodies.shp\"  # Areas to exclude from analysis\n",
    "\n",
    "# Soil and landuse rasters for diagnostic analysis\n",
    "SOIL_RASTER = r\"path/to/your/soil_raster.tif\"\n",
    "LANDUSE_RASTER = r\"path/to/your/landuse_raster.tif\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = r\"path/to/output/directory\"\n",
    "\n",
    "# Performance thresholds (%)\n",
    "PERFORMANCE_THRESHOLDS = [10, 25, 50]  # Good: 0-10%, Moderate: 10-25%, Poor: 25-50%, Very Poor: >50%\n",
    "\n",
    "# Minimum region size (pixels) for poor performance analysis\n",
    "MIN_REGION_SIZE = 20  # ~20 kmÂ² at 1km resolution\n",
    "\n",
    "# Spatial smoothing window size\n",
    "SMOOTHING_WINDOW = 7 # 7x7 pixel moving average (adjust as needed)\n",
    "\n",
    "# Soil type names (Soil ID: Soil Type)\n",
    "SOIL_NAMES = {\n",
    "    0: 'Clay',\n",
    "    1: 'Sandy Clay',\n",
    "    2: 'Clay Loam',\n",
    "    # Add your soil types here\n",
    "}\n",
    "\n",
    "# Landuse names (Landuse ID: Landuse Type)\n",
    "LANDUSE_NAMES = {\n",
    "    20: 'Rangeland - Brush',\n",
    "    30: 'Rangeland - Grasses',\n",
    "    41: 'Agricultural Land - Row Crops',\n",
    "    42: 'Agricultural Land - Sugarcane',\n",
    "    # Add your landuse types here\n",
    "}\n",
    "\n",
    "# ===========================\n",
    "# FUNCTIONS\n",
    "# ===========================\n",
    "\n",
    "def clip_raster(raster_path, shape_path=None):\n",
    "    \"\"\"Load and optionally clip raster with shapefile\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        if shape_path:\n",
    "            shapes_gdf = gpd.read_file(shape_path)\n",
    "            if shapes_gdf.crs != src.crs:\n",
    "                shapes_gdf = shapes_gdf.to_crs(src.crs)\n",
    "            \n",
    "            shapes = [geom for geom in shapes_gdf.geometry if geom is not None]\n",
    "            if shapes:\n",
    "                out_image, out_transform = mask(src, shapes, invert=True, crop=False, \n",
    "                                               nodata=np.nan, all_touched=True)\n",
    "                return out_image[0], src.bounds, src.crs, out_transform\n",
    "        \n",
    "        return src.read(1), src.bounds, src.crs, src.transform\n",
    "\n",
    "def resample_raster(src_path, reference_raster, reference_transform, reference_crs):\n",
    "    \"\"\"Resample raster to match reference resolution and extent\"\"\"\n",
    "    with rasterio.open(src_path) as src:\n",
    "        resampled = np.zeros_like(reference_raster, dtype=src.dtypes[0])\n",
    "        reproject(\n",
    "            source=src.read(1),\n",
    "            destination=resampled,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=reference_transform,\n",
    "            dst_crs=reference_crs,\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "        return resampled\n",
    "\n",
    "def calculate_spatial_mare(observed, predicted):\n",
    "    \"\"\"Calculate pixel-wise Mean Absolute Relative Error\"\"\"\n",
    "    mask = ~(np.isnan(observed) | np.isnan(predicted) | \n",
    "             (observed <= 0) | (predicted <= 0))\n",
    "    \n",
    "    spatial_mare = np.full_like(observed, np.nan, dtype=float)\n",
    "    spatial_mare[mask] = np.abs((observed[mask] - predicted[mask]) / observed[mask]) * 100\n",
    "    \n",
    "    return spatial_mare\n",
    "\n",
    "def spatial_average(array, window_size):\n",
    "    \"\"\"Apply spatial smoothing using moving window average\"\"\"\n",
    "    smoothed = np.copy(array)\n",
    "    valid_mask = ~np.isnan(array)\n",
    "    \n",
    "    temp_array = np.nan_to_num(array, nan=0.0)\n",
    "    filtered_data = uniform_filter(temp_array, size=window_size, mode='constant', cval=0.0)\n",
    "    valid_count = uniform_filter(valid_mask.astype(float), size=window_size, \n",
    "                               mode='constant', cval=0.0)\n",
    "    \n",
    "    valid_count[valid_count == 0] = 1\n",
    "    smoothed = filtered_data / valid_count\n",
    "    smoothed[~valid_mask] = np.nan\n",
    "    \n",
    "    return smoothed\n",
    "\n",
    "def classify_performance(mare_array, thresholds):\n",
    "    \"\"\"Classify areas by performance level\"\"\"\n",
    "    classified = np.full_like(mare_array, np.nan)\n",
    "    valid_mask = ~np.isnan(mare_array)\n",
    "    \n",
    "    classified[valid_mask] = 0  # Good\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        classified[valid_mask & (mare_array > threshold)] = i + 1\n",
    "    \n",
    "    return classified\n",
    "\n",
    "\n",
    "\n",
    "def find_poor_regions(performance_classes, min_size=20):\n",
    "    \"\"\"\n",
    "    Find connected regions in the poor and very poor performance classes\n",
    "    \n",
    "    Parameters:\n",
    "    performance_classes (numpy.ndarray): Classification array\n",
    "    min_size (int): Minimum region size to keep\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with region information including bounding boxes\n",
    "    \"\"\"\n",
    "    regions = {}\n",
    "    \n",
    "    # Only interested in poor (2) and very poor (3) classes\n",
    "    for cls in [2, 3]:\n",
    "        # Create binary mask for this class\n",
    "        binary = (performance_classes == cls)\n",
    "        \n",
    "        # Find connected components\n",
    "        labeled_array, num_features = label(binary)\n",
    "        \n",
    "        # Extract regions\n",
    "        regions[cls] = []\n",
    "        \n",
    "        if num_features > 0:\n",
    "            # Get unique labels (excluding 0 which is background)\n",
    "            unique_labels = np.unique(labeled_array)\n",
    "            unique_labels = unique_labels[unique_labels > 0]\n",
    "            \n",
    "            for i in unique_labels:\n",
    "                # Create mask for this region\n",
    "                region_mask = (labeled_array == i)\n",
    "                region_size = np.sum(region_mask)\n",
    "                \n",
    "                if region_size >= min_size:\n",
    "                    # Find the centroid\n",
    "                    rows, cols = np.where(region_mask)\n",
    "                    centroid = (int(np.mean(rows)), int(np.mean(cols)))\n",
    "                    \n",
    "                    # Find the bounding box\n",
    "                    if len(rows) > 0 and len(cols) > 0:\n",
    "                        min_row, max_row = np.min(rows), np.max(rows)\n",
    "                        min_col, max_col = np.min(cols), np.max(cols)\n",
    "                        bbox = (int(min_row), int(min_col), int(max_row), int(max_col))\n",
    "                        \n",
    "                        regions[cls].append({\n",
    "                            'id': int(i),\n",
    "                            'size': int(region_size),\n",
    "                            'centroid': centroid,\n",
    "                            'bbox': bbox,\n",
    "                            'mask': region_mask\n",
    "                        })\n",
    "    \n",
    "    return regions\n",
    "\n",
    "def analyze_regions_soil_landuse(regions, soil_data, landuse_data):\n",
    "    \"\"\"Analyze soil and landuse composition of poor regions\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for cls, region_list in regions.items():\n",
    "        results[cls] = []\n",
    "        \n",
    "        for region in region_list:\n",
    "            region_mask = region['mask']\n",
    "            \n",
    "            # Extract data for this region\n",
    "            soil_in_region = soil_data[region_mask]\n",
    "            landuse_in_region = landuse_data[region_mask]\n",
    "            \n",
    "            # Remove NaN values\n",
    "            valid_soil = soil_in_region[~np.isnan(soil_in_region)]\n",
    "            valid_landuse = landuse_in_region[~np.isnan(landuse_in_region)]\n",
    "            \n",
    "            # Find dominant types\n",
    "            if len(valid_soil) > 0:\n",
    "                soil_vals, soil_counts = np.unique(valid_soil, return_counts=True)\n",
    "                dominant_soil_idx = np.argmax(soil_counts)\n",
    "                dominant_soil = int(soil_vals[dominant_soil_idx])\n",
    "                dominant_soil_pct = (soil_counts[dominant_soil_idx] / len(valid_soil)) * 100\n",
    "            else:\n",
    "                dominant_soil, dominant_soil_pct = None, 0\n",
    "            \n",
    "            if len(valid_landuse) > 0:\n",
    "                lu_vals, lu_counts = np.unique(valid_landuse, return_counts=True)\n",
    "                dominant_lu_idx = np.argmax(lu_counts)\n",
    "                dominant_landuse = int(lu_vals[dominant_lu_idx])\n",
    "                dominant_landuse_pct = (lu_counts[dominant_lu_idx] / len(valid_landuse)) * 100\n",
    "            else:\n",
    "                dominant_landuse, dominant_landuse_pct = None, 0\n",
    "            \n",
    "            results[cls].append({\n",
    "                'region_id': region['id'],\n",
    "                'size': region['size'],\n",
    "                'dominant_soil': dominant_soil,\n",
    "                'dominant_soil_pct': dominant_soil_pct,\n",
    "                'dominant_landuse': dominant_landuse,\n",
    "                'dominant_landuse_pct': dominant_landuse_pct\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_raster(data, reference_path, output_path, transform):\n",
    "    \"\"\"Save array as georeferenced raster\"\"\"\n",
    "    with rasterio.open(reference_path) as src:\n",
    "        profile = src.profile.copy()\n",
    "        profile.update({\n",
    "            'transform': transform,\n",
    "            'dtype': 'float32',\n",
    "            'nodata': np.nan\n",
    "        })\n",
    "        \n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(data.astype('float32'), 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# MAIN ANALYSIS\n",
    "# ===========================\n",
    "\n",
    "def main():\n",
    "    print(\"SWAT+ ET Spatial Performance Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    print(\"Loading reference ET data...\")\n",
    "    ref_et, ref_bounds, ref_crs, ref_transform = clip_raster(WAPOR_ET_PATH, WATERBODIES_MASK)\n",
    "    ref_et = np.where(ref_et <= 0, np.nan, ref_et)\n",
    "    \n",
    "    print(\"Loading SWAT+ ET data...\")\n",
    "    swat_et, _, _, _ = clip_raster(SWAT_ET_PATH, WATERBODIES_MASK)\n",
    "    swat_et = np.where(swat_et <= 0, np.nan, swat_et)\n",
    "    \n",
    "    # Resample SWAT+ to match reference\n",
    "    print(\"Resampling SWAT+ ET to match reference resolution...\")\n",
    "    swat_et_resampled = resample_raster(SWAT_ET_PATH, ref_et, ref_transform, ref_crs)\n",
    "    swat_et_resampled = np.where(swat_et_resampled <= 0, np.nan, swat_et_resampled)\n",
    "    \n",
    "    # Calculate MARE\n",
    "    print(\"Calculating spatial MARE...\")\n",
    "    spatial_mare = calculate_spatial_mare(ref_et, swat_et_resampled)\n",
    "    \n",
    "    # Apply smoothing\n",
    "    print(f\"Applying spatial smoothing (window size: {SMOOTHING_WINDOW})...\")\n",
    "    smoothed_mare = spatial_average(spatial_mare, SMOOTHING_WINDOW)\n",
    "    \n",
    "    # Classify performance\n",
    "    print(\"Classifying performance...\")\n",
    "    performance_classes = classify_performance(smoothed_mare, PERFORMANCE_THRESHOLDS)\n",
    "    \n",
    "    # Load soil and landuse data\n",
    "    print(\"Loading soil and landuse data...\")\n",
    "    soil_data = resample_raster(SOIL_RASTER, ref_et, ref_transform, ref_crs)\n",
    "    landuse_data = resample_raster(LANDUSE_RASTER, ref_et, ref_transform, ref_crs)\n",
    "    \n",
    "    # Find poor regions\n",
    "    print(f\"Identifying poor performance regions (min size: {MIN_REGION_SIZE} pixels)...\")\n",
    "    poor_regions = find_poor_regions(performance_classes, MIN_REGION_SIZE)\n",
    "    \n",
    "    # Analyze regions\n",
    "    print(\"Analyzing soil-landuse composition of poor regions...\")\n",
    "    region_analysis = analyze_regions_soil_landuse(poor_regions, soil_data, landuse_data)\n",
    "    \n",
    "    # Save outputs\n",
    "    print(\"\\nSaving outputs...\")\n",
    "    save_raster(spatial_mare, WAPOR_ET_PATH, f\"{OUTPUT_DIR}/spatial_mare.tif\", ref_transform)\n",
    "    save_raster(smoothed_mare, WAPOR_ET_PATH, f\"{OUTPUT_DIR}/smoothed_mare.tif\", ref_transform)\n",
    "    save_raster(performance_classes, WAPOR_ET_PATH, f\"{OUTPUT_DIR}/performance_classes.tif\", ref_transform)\n",
    "    \n",
    "    # Create summary report\n",
    "    performance_labels = ['Good', 'Moderate', 'Poor', 'Very Poor']\n",
    "    \n",
    "    # Calculate area statistics\n",
    "    unique_classes, counts = np.unique(performance_classes[~np.isnan(performance_classes)], \n",
    "                                     return_counts=True)\n",
    "    total_pixels = np.sum(counts)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cls, count in zip(unique_classes.astype(int), counts):\n",
    "        percentage = (count / total_pixels) * 100\n",
    "        print(f\"{performance_labels[cls]}: {percentage:.1f}% ({count} pixels)\")\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    all_regions = []\n",
    "    for cls, regions in region_analysis.items():\n",
    "        for region in regions:\n",
    "            region['performance_class'] = performance_labels[cls]\n",
    "            region['dominant_soil_name'] = SOIL_NAMES.get(region['dominant_soil'], 'Unknown')\n",
    "            region['dominant_landuse_name'] = LANDUSE_NAMES.get(region['dominant_landuse'], 'Unknown')\n",
    "            all_regions.append(region)\n",
    "\n",
    "    # Create spatial visualization of regions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot performance classification\n",
    "    cmap = plt.cm.get_cmap('viridis', len(PERFORMANCE_THRESHOLDS) + 1)\n",
    "    plt.imshow(performance_classes, cmap=cmap, vmin=-0.5, vmax=len(PERFORMANCE_THRESHOLDS) + 0.5)\n",
    "\n",
    "    # Overlay region boundaries for poor and very poor regions\n",
    "    for cls in [2, 3]:  # Poor and Very Poor\n",
    "        if cls not in poor_regions:\n",
    "            continue\n",
    "            \n",
    "        for region in poor_regions[cls]:\n",
    "            # Get bounding box\n",
    "            min_row, min_col, max_row, max_col = region['bbox']\n",
    "            \n",
    "            # Plot rectangle\n",
    "            rect = plt.Rectangle((min_col-0.5, min_row-0.5), max_col-min_col+1, max_row-min_row+1, \n",
    "                                edgecolor='red' if cls == 3 else 'orange', \n",
    "                                facecolor='none', linewidth=1.5, alpha=0.7)\n",
    "            plt.gca().add_patch(rect)\n",
    "            \n",
    "            # Add region ID text\n",
    "            plt.text(region['centroid'][1], region['centroid'][0], str(region['id']), \n",
    "                    color='white', fontsize=8, ha='center', va='center',\n",
    "                    bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.1'))\n",
    "    \n",
    "    cbar = plt.colorbar(ticks=range(len(PERFORMANCE_THRESHOLDS) + 1))\n",
    "    cbar.set_label('Performance Class')\n",
    "    cbar.set_ticklabels(performance_labels)\n",
    "    plt.title('Poor Performance Regions with Region IDs')\n",
    "    plt.axis('on')\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/poor_regions_map.png\", dpi=300) \n",
    "    plt.show()\n",
    "    \n",
    "    if all_regions:\n",
    "        df = pd.DataFrame(all_regions)\n",
    "        df.to_csv(f\"{OUTPUT_DIR}/poor_regions_analysis.csv\", index=False)\n",
    "        print(f\"\\nDetailed analysis saved to: {OUTPUT_DIR}/poor_regions_analysis.csv\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm (Python 3.11.13)",
   "language": "python",
   "name": "vlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
